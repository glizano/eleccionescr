# Backend Configuration

# LLM Provider Selection: "google" or "openai"
LLM_PROVIDER=google

# Google AI (required if LLM_PROVIDER=google)
GOOGLE_API_KEY=your_google_api_key_here
GOOGLE_MODEL=gemini-2.5-flash
# Safety threshold: BLOCK_NONE, BLOCK_ONLY_HIGH, BLOCK_MEDIUM_AND_ABOVE (default), BLOCK_LOW_AND_ABOVE
GOOGLE_SAFETY_THRESHOLD=BLOCK_MEDIUM_AND_ABOVE

# OpenAI (required if LLM_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini

# Qdrant Configuration
QDRANT_URL=http://qdrant:6333

# Backend Service
BACKEND_URL=http://backend:8000
DEBUG=true

# Langfuse (LLM Observability)
# For local development, use the local Langfuse instance
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
LANGFUSE_HOST=http://langfuse:3000
LANGFUSE_ENABLED=false
# To enable: Set LANGFUSE_ENABLED=true and configure keys after creating a project in Langfuse UI (http://localhost:3000)

# Langfuse Server Configuration (optional, only for production)
# LANGFUSE_NEXTAUTH_SECRET=generate-a-random-secret-here
# LANGFUSE_SALT=generate-another-random-salt-here
