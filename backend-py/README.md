# EleccionesCR 2026 - Backend con Agentes

Backend inteligente basado en agentes con LangGraph para el chatbot RAG de planes de gobierno.

## ğŸ¤– Sistema de Agentes

El backend utiliza **LangGraph** para orquestar un flujo inteligente de agentes:

```
User Question
     â†“
[Intent Classifier] â† Detecta si es pregunta especÃ­fica o general
     â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                  â”‚
[Specific]      [General]
     â†“                â†“
[Party Extractor]  [RAG sin filtro]
     â†“                â†“
[RAG filtrado]       â”‚
     â†“                â†“
[Response Generator] â†â”˜
     â†“
  Response + Trace
```

## âœ¨ CaracterÃ­sticas

- âœ… **Routing inteligente**: Identifica automÃ¡ticamente si la pregunta es sobre un partido especÃ­fico
- âœ… **ExtracciÃ³n de entidades**: Detecta nombres de partidos (PLN, PUSC, PNR, FA, etc.)
- âœ… **RAG contextual**: BÃºsquedas filtradas cuando es apropiado
- âœ… **Trazabilidad completa**: Cada respuesta incluye el trace del agente
- âœ… **LangChain + LangGraph**: OrquestaciÃ³n profesional de agentes
- âœ… **100% local**: No requiere servicios externos para desarrollo

## ğŸš€ Setup

### 1. Instalar dependencias

**Para desarrollo local** (incluye sentence-transformers para embeddings gratis):
```bash
uv sync --group dev
```

**Solo producciÃ³n** (excluye dependencias pesadas):
```bash
uv sync
```

> ğŸ’¡ **Nota**: Las dependencias de ML pesadas (sentence-transformers, PyTorch) estÃ¡n en el grupo `dev` para mantener la imagen Docker de producciÃ³n <1 GB. Ver [DEPLOYMENT_OPTIMIZATION.md](../DEPLOYMENT_OPTIMIZATION.md) para mÃ¡s detalles.

### 2. Configurar variables de entorno

```bash
cp .env.example .env
# Edita .env y configura GOOGLE_API_KEY
```

### 3. Asegurar que Qdrant estÃ© corriendo

```bash
docker ps | grep qdrant
```

### 4. Iniciar el servidor

```bash
uv run python run.py
```

El servidor estarÃ¡ en `http://localhost:8000`

## ğŸ“ Uso

### Pregunta especÃ­fica de partido (tema concreto)

```bash
curl -X POST http://localhost:8000/api/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Â¿QuÃ© propone el PLN sobre educaciÃ³n?"
  }'
```

**Trace del agente:**
```json
{
  "agent_trace": {
    "intent": "specific_party",
    "parties_detected": ["PLN"],
    "chunks_retrieved": 5,
    "steps": [
      "Intent: specific_party",
      "Parties: ['PLN']",
      "Retrieved 5 chunks",
      "Response generated"
    ]
  }
}
```

### Pregunta de plan completo de partido (nuevo)

```bash
curl -X POST http://localhost:8000/api/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Â¿QuÃ© plantea el plan del PLN?"
  }'
```

**Trace del agente:**
```json
{
  "agent_trace": {
    "intent": "party_general_plan",
    "parties_detected": ["PLN"],
    "chunks_retrieved": 15,
    "steps": [
      "Intent: party_general_plan",
      "Parties: ['PLN']",
      "Retrieved 15 chunks",
      "Response generated"
    ]
  }
}
```

### Pregunta general/comparativa

```bash
curl -X POST http://localhost:8000/api/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Â¿QuÃ© proponen los partidos sobre salud?"
  }'
```

**Trace del agente:**
```json
{
  "agent_trace": {
    "intent": "general_comparison",
    "parties_detected": [],
    "chunks_retrieved": 5,
    "steps": [
      "Intent: general_comparison",
      "Retrieved 5 chunks",
      "Response generated"
    ]
  }
}
```

## ğŸ“š DocumentaciÃ³n Interactiva

Visita `http://localhost:8000/docs` para la documentaciÃ³n Swagger UI.

## ğŸ—ï¸ Arquitectura de Agentes

### Intent Classifier Agent
Clasifica la pregunta en:
- `specific_party`: Pregunta sobre un tema especÃ­fico de un partido (ej: "Â¿QuÃ© propone el PLN sobre educaciÃ³n?")
- `party_general_plan`: Pregunta que solicita un resumen completo del plan de un partido (ej: "Â¿QuÃ© plantea el plan del PLN?")
- `general_comparison`: Pregunta general o comparativa entre partidos
- `unclear`: No estÃ¡ claro

### Party Extractor Agent
Extrae nombres de partidos mencionados usando LLM con few-shot examples.

Partidos conocidos (20 partidos): ACRM, CAC, CDS, CR1, FA, PA, PDLCT, PEL, PEN, PIN, PJSC, PLN, PLP, PNG, PNR, PPSO, PSD, PUCD, PUSC, UP

### RAG Agent
Ejecuta bÃºsqueda vectorial con estrategias adaptativas:
- **5 chunks filtrados** por partido si intent = specific_party (temas especÃ­ficos)
- **15 chunks filtrados** por partido si intent = party_general_plan (resumen completo)
- **10 chunks balanceados** entre partidos si intent = general_comparison (2 por partido)

### Response Generator Agent
Genera respuesta final con:
- Citas de fuentes
- Formato estructurado
- ValidaciÃ³n de informaciÃ³n

## ğŸ”§ ConfiguraciÃ³n

Variables en `.env`:

```bash
QDRANT_URL=http://localhost:6333
PORT=8000
DEBUG=true

# LLM Provider Selection: "google" or "openai"
LLM_PROVIDER=google

# Google AI (required if LLM_PROVIDER=google)
GOOGLE_API_KEY=tu_key_aqui
GOOGLE_MODEL=gemini-2.5-flash
# Safety threshold: BLOCK_NONE, BLOCK_ONLY_HIGH, BLOCK_MEDIUM_AND_ABOVE (default), BLOCK_LOW_AND_ABOVE
GOOGLE_SAFETY_THRESHOLD=BLOCK_MEDIUM_AND_ABOVE

# OpenAI (required if LLM_PROVIDER=openai)
OPENAI_API_KEY=tu_openai_key_aqui
OPENAI_MODEL=gpt-4o-mini

# Rate Limiting (para controlar costos de LLM)
# LÃ­mites por direcciÃ³n IP
MAX_REQUESTS_PER_MINUTE=10
MAX_REQUESTS_PER_HOUR=30
MAX_REQUESTS_PER_DAY=100
```

### Proveedores de LLM Soportados

El sistema usa **LangChain** para abstraer los proveedores de LLM, soportando:

| Proveedor | Variable de entorno | Modelos disponibles | LangChain Class |
|-----------|---------------------|---------------------|-----------------|
| Google Gemini | `LLM_PROVIDER=google` | gemini-2.5-flash (default), gemini-1.5-pro, etc. | `ChatGoogleGenerativeAI` |
| OpenAI | `LLM_PROVIDER=openai` | gpt-4o-mini (default), gpt-4o, gpt-4-turbo, etc. | `ChatOpenAI` |

Para cambiar de proveedor, simplemente modifica `LLM_PROVIDER` en tu archivo `.env` y proporciona la API key correspondiente.

#### Agregar un Nuevo Proveedor

Gracias a las abstracciones de LangChain, es fÃ¡cil agregar nuevos proveedores (Anthropic Claude, Cohere, etc.):

1. Agrega la dependencia de LangChain para el proveedor (ej: `langchain-anthropic`)
2. Crea una funciÃ³n `create_provider()` en `app/services/llm_providers/`
3. Actualiza la factory en `factory.py`

### ConfiguraciÃ³n de Seguridad (Google Gemini)

Para Google Gemini, puedes configurar el nivel de filtros de seguridad con `GOOGLE_SAFETY_THRESHOLD`:

- `BLOCK_MEDIUM_AND_ABOVE` (default): Bloquea contenido con nivel medio o superior (recomendado)
- `BLOCK_ONLY_HIGH`: Solo bloquea contenido de alto riesgo
- `BLOCK_LOW_AND_ABOVE`: Bloquea incluso contenido de bajo riesgo (mÃ¡s restrictivo)
- `BLOCK_NONE`: Desactiva los filtros de seguridad (no recomendado para producciÃ³n)

### Rate Limiting para Servicio PÃºblico

Este backend estÃ¡ diseÃ±ado para ser **pÃºblico y accesible** sin barreras de autenticaciÃ³n, pero con **protecciÃ³n contra uso excesivo** para controlar los costos de LLM.

#### Sistema de Rate Limiting por IP

El rate limiting estÃ¡ **siempre habilitado** con mÃºltiples niveles de protecciÃ³n:

- **Por minuto**: `MAX_REQUESTS_PER_MINUTE=10` (default: 10 requests/minuto)
- **Por hora**: `MAX_REQUESTS_PER_HOUR=30` (default: 30 requests/hora)
- **Por dÃ­a**: `MAX_REQUESTS_PER_DAY=100` (default: 100 requests/dÃ­a)

El lÃ­mite se aplica por **direcciÃ³n IP**, permitiendo acceso pÃºblico pero previniendo abuso.

#### CÃ³mo Funciona

1. **Sin autenticaciÃ³n requerida**: Los usuarios pueden usar el servicio directamente
2. **Tracking por IP**: Se rastrea el uso por direcciÃ³n IP del cliente
3. **MÃºltiples ventanas de tiempo**: ProtecciÃ³n a corto (minuto), mediano (hora) y largo plazo (dÃ­a)
4. **Integrado con Langfuse**: Todo el uso se registra para anÃ¡lisis de costos
5. **Respuesta 429**: Cuando se excede un lÃ­mite, se retorna HTTP 429 (Too Many Requests)

#### Ejemplo de Uso

```bash
# Uso normal - sin headers especiales requeridos
curl -X POST http://localhost:8000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "Â¿QuÃ© propone el PLN sobre educaciÃ³n?"}'

# Con session_id para tracking en Langfuse
curl -X POST http://localhost:8000/api/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Â¿QuÃ© propone el PLN sobre educaciÃ³n?",
    "session_id": "user-browser-session-123"
  }'
```

#### Ajustar LÃ­mites

Para modificar los lÃ­mites segÃºn tu presupuesto de LLM:

```bash
# Para desarrollo/testing (lÃ­mites mÃ¡s altos)
MAX_REQUESTS_PER_MINUTE=50
MAX_REQUESTS_PER_HOUR=200
MAX_REQUESTS_PER_DAY=1000

# Para producciÃ³n con presupuesto limitado (mÃ¡s restrictivo)
MAX_REQUESTS_PER_MINUTE=5
MAX_REQUESTS_PER_HOUR=15
MAX_REQUESTS_PER_DAY=50
```

#### Monitoreo con Langfuse

Todos los requests se registran en Langfuse (si estÃ¡ habilitado) con:
- Session ID del usuario
- Metadata de costos por request
- AnÃ¡lisis de uso por IP/sesiÃ³n
- MÃ©tricas de rate limiting

Esto permite monitorear costos reales y ajustar lÃ­mites segÃºn necesidad.

## ğŸ“Š Ventajas vs VersiÃ³n Anterior

âœ… **Inteligencia real**: Los agentes toman decisiones contextuales  
âœ… **Filtrado automÃ¡tico**: No mÃ¡s sources de partidos incorrectos  
âœ… **Trazabilidad**: Cada decisiÃ³n es visible en el trace  
âœ… **Escalable**: FÃ¡cil agregar nuevos agentes  
âœ… **Debuggeable**: Logs detallados de cada paso  

## ğŸ§ª Testing

```bash
# Health check
curl http://localhost:8000/health

# List known parties
curl http://localhost:8000/api/parties
```

## ğŸ§ª Testing

### Ejecutar tests localmente

```bash
# Instalar dependencias de desarrollo
uv sync --group dev

# Ejecutar todos los tests
uv run pytest

# Con cobertura
uv run pytest --cov=app --cov-report=html

# Ver reporte de cobertura
open htmlcov/index.html
```

### Ejecutar checks de CI localmente

```bash
# Ejecutar todos los checks que se ejecutan en CI
./scripts/ci-check.sh
```

## ğŸ” Linting y Formateo

Este proyecto usa **ruff** para linting y formateo:

```bash
# Check linting
uv run ruff check .

# Fix automÃ¡ticamente
uv run ruff check --fix .

# Check formato
uv run ruff format --check .

# Formatear cÃ³digo
uv run ruff format .
```

## ğŸ³ Docker

### Desarrollo con Docker Compose

```bash
# Iniciar todos los servicios (backend + Qdrant)
docker-compose up -d

# Ver logs
docker-compose logs -f backend

# Detener servicios
docker-compose down
```

### Build manual de imagen Docker

```bash
# Build
docker build -t backend-py:latest .

# Run
docker run -p 8000:8000 \
  -e QDRANT_URL=http://qdrant:6333 \
  -e GOOGLE_API_KEY=your_key \
  backend-py:latest
```

## ğŸš€ CI/CD

El proyecto incluye GitHub Actions para:

- âœ… **Linting**: Verifica calidad de cÃ³digo con ruff
- âœ… **Tests**: Ejecuta suite de tests con pytest
- âœ… **Build**: Valida que el cÃ³digo se puede importar
- âœ… **Deploy**: Placeholder para deployment automÃ¡tico

Ver [`.github/workflows/README_BACKEND.md`](../../.github/workflows/README_BACKEND.md) para mÃ¡s detalles.

### Secrets requeridos en GitHub

- `GOOGLE_API_KEY`: Para tests que usan el LLM
- `QDRANT_URL`: URL de Qdrant en producciÃ³n (para deployment)
- `QDRANT_API_KEY`: API key de Qdrant (para deployment)

## ğŸ”œ PrÃ³ximos pasos

- [ ] Integrar LangSmith para visualizaciÃ³n de traces
- [ ] Agregar agent de fact-checking
- [ ] Implementar memoria conversacional
- [ ] Cache con Redis
- [ ] MÃ©tricas de accuracy por agente
